# AgentEval

AgentEval is a framework that enables the automatic assessment of the utility of LLM-powered applications.

![AgentEval Overview](https://github.com/narabzad/agenteval/blob/main/agenteval_overview.png)

CriticAgent creates a set of criteria and suggested values. QuantifierAgent quantifies the criteria for a considered application, and VerifierAgent verifies the criteria based on its robustness. The output of the QuantifierAgent is a multi-dimensional task utility of the system.

# File Structure

    .
    ├── logs                                                       # Input logs (task output of LLM powered applications)
    │   ├── math_logs                                              # Math Problem Solving task (subfolders are for different LLM benchmarks)
    │   │   ├── autogen          
    │   │   ├── react         
    │   │   ├── vanilla_solver 
    │   │   ├── sample_math_response_failed.txt                    # Samples that will be used in prompts
    │   │   └── sample_math_response_successful.txt                # ALFWorld task (subfolders are for different LLM benchmarks)
    │   └── alfworld_logs 
    │       ├── multiagent                 
    │       └── twoagent  
    ├── outputs                                                    # Output files (data files generated by AgentEval)
    ├── agenteval_verifier_full_experiment_alfworld.ipynb          # code for running AgentEval experiment on the ALFWorld dataset
    └── agenteval_verifier_full_experiment_math.ipynb              # code for running AgentEval experiment on the Math Problems dataset

# Blog Posts:
- [How to Assess Utility of LLM-powered Applications?](https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval/)

- [AgentEval: A Developer Tool to Assess Utility of LLM-powered Applications](https://microsoft.github.io/autogen/blog/2024/06/21/AgentEval/)
